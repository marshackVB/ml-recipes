{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1DCovnet\n",
    "Using a 1DCovnet combined with word embeddings for text classification. Download the [Glove word embeddings](https://nlp.stanford.edu/projects/glove/)(glove.6B.zip). Also, download the raw [IMDB](http://mng.bz/0tIo) dataset and uncompress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import layers, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 100 dimension word embeddings\n",
    "glove_dir = \"/Users/marshall.carter/Documents/glove\"\n",
    "\n",
    "imdb_dir = \"/Users/marshall.carter/Documents/my_repos/keras/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form. \n",
      "\n",
      "0 Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we're from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample the data\n",
    "for i in range(2):\n",
    "    print(labels[i], texts[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of records\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text feature parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off reviews as this many words\n",
    "maxlen = 200\n",
    "\n",
    "# Set number of observation for validation holdout\n",
    "training_samples = 15000\n",
    "validation_samples = 10000\n",
    "\n",
    "# Consider on the top n (max_words) number of words in the model\n",
    "max_words = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash the words to an integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[777, 16, 28, 4, 1, 115, 2278, 6887, 11, 19, 1025, 5, 27, 19499, 5, 42, 2425, 1861, 128, 2270, 5, 3, 6985, 308, 7, 7, 3383, 2373, 1, 19, 36, 463, 16115, 3169, 2, 222, 3, 1016, 174, 20, 49, 808]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1\n",
      "and 2\n",
      "a 3\n",
      "of 4\n",
      "to 5\n"
     ]
    }
   ],
   "source": [
    "# Word to integer mapping\n",
    "for word, index in list(word_index.items())[:5]:\n",
    "    print(word, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rever to index such that words are keys\n",
    "reverse_index = {value: key for key, value in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working', 'with', 'one', 'of', 'the', 'best', 'shakespeare', 'sources', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', \"it's\", 'source', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', 'br', 'br', 'branagh', 'steals', 'the', 'film', 'from', 'under', \"fishburne's\", 'nose', 'and', \"there's\", 'a', 'talented', 'cast', 'on', 'good', 'form']\n"
     ]
    }
   ],
   "source": [
    "print([reverse_index[index_val] for index_val in sequences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the integer sequences so they are the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Create labels array\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,   777,    16,    28,     4,\n",
       "           1,   115,  2278,  6887,    11,    19,  1025,     5,    27,\n",
       "       19499,     5,    42,  2425,  1861,   128,  2270,     5,     3,\n",
       "        6985,   308,     7,     7,  3383,  2373,     1,    19,    36,\n",
       "         463, 16115,  3169,     2,   222,     3,  1016,   174,    20,\n",
       "          49,   808], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and shuffle the data; creating training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations\n",
    "indices = np.arange(data.shape[0])\n",
    "\n",
    "# Randomly shuffle observations\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the GLOVE word embedding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "# Read in as a list of vectors\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "# Populate the embedding matrix\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        # Get the word's GLOVE vector\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        # Return a 0 if the word has no GLOVE vector\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is expected; index[0] \"is a placeholder\"\n",
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the 1DCovnet architecture\n",
    "Using the function API of Keras to specify convolutional layers with multiple kernel sizes. We will then specify an achitecture similiar to the below, which is described in this [paper](https://arxiv.org/abs/1510.03820)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*0efgxnFIaLTZ2qkY\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "review (InputLayer)             (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 200, 100)     2000000     review[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 199, 100)     20100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 198, 100)     30100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 197, 100)     40100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 100)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 100)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 100)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 300)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            301         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,090,601\n",
      "Trainable params: 90,601\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_filters = 100\n",
    "\n",
    "text_input = Input(shape=(maxlen,), dtype='int32', name='review')\n",
    "\n",
    "embedded_text = layers.Embedding(max_words,\n",
    "                                100,\n",
    "                                input_length = maxlen,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)(text_input)\n",
    "\n",
    "conv1d_2 = layers.Conv1D(n_filters, 2, activation='relu')(embedded_text)\n",
    "maxpooling_2 = layers.GlobalMaxPooling1D()(conv1d_2)\n",
    "\n",
    "conv1d_3 = layers.Conv1D(n_filters, 3, activation='relu')(embedded_text)\n",
    "maxpooling_3 = layers.GlobalMaxPooling1D()(conv1d_3)\n",
    "\n",
    "conv1d_4 = layers.Conv1D(n_filters, 4, activation='relu')(embedded_text)\n",
    "maxpooling_4 = layers.GlobalMaxPooling1D()(conv1d_4)\n",
    "\n",
    "concat_maxpoolings = layers.concatenate([maxpooling_2, maxpooling_3, maxpooling_4], axis=-1)\n",
    "\n",
    "prediction = layers.Dense(1, activation='sigmoid')(concat_maxpoolings)\n",
    "\n",
    "model = Model(text_input, prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 0.1704 - accuracy: 0.9473 - val_loss: 0.3031 - val_accuracy: 0.8705\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.1400 - accuracy: 0.9629 - val_loss: 0.3030 - val_accuracy: 0.8696\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.1087 - accuracy: 0.9791 - val_loss: 0.3113 - val_accuracy: 0.8657\n",
      "Epoch 4/10\n",
      " - 32s - loss: 0.0872 - accuracy: 0.9881 - val_loss: 0.3074 - val_accuracy: 0.8716\n",
      "Epoch 5/10\n",
      " - 33s - loss: 0.0687 - accuracy: 0.9934 - val_loss: 0.3364 - val_accuracy: 0.8614\n",
      "Epoch 6/10\n",
      " - 33s - loss: 0.0543 - accuracy: 0.9973 - val_loss: 0.3146 - val_accuracy: 0.8709\n",
      "Epoch 7/10\n",
      " - 32s - loss: 0.0424 - accuracy: 0.9985 - val_loss: 0.3227 - val_accuracy: 0.8701\n",
      "Epoch 8/10\n",
      " - 33s - loss: 0.0322 - accuracy: 0.9997 - val_loss: 0.3312 - val_accuracy: 0.8707\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.0259 - accuracy: 0.9999 - val_loss: 0.3337 - val_accuracy: 0.8701\n",
      "Epoch 10/10\n",
      " - 32s - loss: 0.0204 - accuracy: 0.9999 - val_loss: 0.3515 - val_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    batch_size=100, \n",
    "                    validation_data=(x_val, y_val), \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dense layers to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "review (InputLayer)             (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 200, 100)     2000000     review[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 199, 100)     20100       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 198, 100)     30100       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 197, 100)     40100       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 100)          0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 100)          0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 100)          0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 300)          0           global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100)          30100       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 50)           5050        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 50)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            51          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,125,501\n",
      "Trainable params: 125,501\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_filters = 100\n",
    "\n",
    "text_input = Input(shape=(maxlen,), dtype='int32', name='review')\n",
    "\n",
    "embedded_text = layers.Embedding(max_words,\n",
    "                                100,\n",
    "                                input_length = maxlen,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = False)(text_input)\n",
    "\n",
    "conv1d_2 = layers.Conv1D(n_filters, 2, activation='relu')(embedded_text)\n",
    "maxpooling_2 = layers.GlobalMaxPooling1D()(conv1d_2)\n",
    "\n",
    "conv1d_3 = layers.Conv1D(n_filters, 3, activation='relu')(embedded_text)\n",
    "maxpooling_3 = layers.GlobalMaxPooling1D()(conv1d_3)\n",
    "\n",
    "conv1d_4 = layers.Conv1D(n_filters, 4, activation='relu')(embedded_text)\n",
    "maxpooling_4 = layers.GlobalMaxPooling1D()(conv1d_4)\n",
    "\n",
    "concat_maxpoolings = layers.concatenate([maxpooling_2, maxpooling_3, maxpooling_4], axis=-1)\n",
    "\n",
    "dense_1 = layers.Dense(100, activation='relu')(concat_maxpoolings)\n",
    "dense_1_dropout = layers.Dropout(0.5)(dense_1)\n",
    "\n",
    "dense_2 = layers.Dense(50, activation='relu')(dense_1_dropout)\n",
    "dense_2_dropout = layers.Dropout(0.5)(dense_2)\n",
    "\n",
    "prediction = layers.Dense(1, activation='sigmoid')(dense_2_dropout)\n",
    "\n",
    "model = Model(text_input, prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 32s - loss: 0.6907 - accuracy: 0.5416 - val_loss: 0.6142 - val_accuracy: 0.7400\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.5363 - accuracy: 0.7133 - val_loss: 0.4012 - val_accuracy: 0.8323\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.4290 - accuracy: 0.7931 - val_loss: 0.3968 - val_accuracy: 0.8453\n",
      "Epoch 4/10\n",
      " - 32s - loss: 0.3766 - accuracy: 0.8219 - val_loss: 0.3672 - val_accuracy: 0.8417\n",
      "Epoch 5/10\n",
      " - 32s - loss: 0.3462 - accuracy: 0.8369 - val_loss: 0.3377 - val_accuracy: 0.8521\n",
      "Epoch 6/10\n",
      " - 34s - loss: 0.3088 - accuracy: 0.8593 - val_loss: 0.3479 - val_accuracy: 0.8446\n",
      "Epoch 7/10\n",
      " - 33s - loss: 0.2932 - accuracy: 0.8491 - val_loss: 0.3689 - val_accuracy: 0.8520\n",
      "Epoch 8/10\n",
      " - 32s - loss: 0.2335 - accuracy: 0.8865 - val_loss: 0.3388 - val_accuracy: 0.8583\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.1984 - accuracy: 0.8971 - val_loss: 0.3758 - val_accuracy: 0.8582\n",
      "Epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    batch_size=100, \n",
    "                    validation_data=(x_val, y_val), \n",
    "                    verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
